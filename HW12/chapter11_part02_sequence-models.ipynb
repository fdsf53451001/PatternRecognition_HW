{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### B0829024 HW12"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "### Processing words as a sequence: The sequence model approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "#### A first practical example"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "**Downloading the data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "# !curl -O https://ai.stanford.edu/~amaas/data/sentiment/aclImdb_v1.tar.gz\n",
    "# !tar -xf aclImdb_v1.tar.gz\n",
    "# !rm -r aclImdb/train/unsup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "**Preparing the data**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 52528 files belonging to 3 classes.\n",
      "Found 22472 files belonging to 2 classes.\n",
      "Found 25000 files belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "import os, pathlib, shutil, random\n",
    "from tensorflow import keras\n",
    "batch_size = 32\n",
    "base_dir = pathlib.Path(\"aclImdb\")\n",
    "val_dir = base_dir / \"val\"\n",
    "train_dir = base_dir / \"train\"\n",
    "for category in (\"neg\", \"pos\"):\n",
    "    os.makedirs(val_dir / category,  exist_ok=True)\n",
    "    files = os.listdir(train_dir / category)\n",
    "    random.Random(1337).shuffle(files)\n",
    "    num_val_samples = int(0.2 * len(files))\n",
    "    val_files = files[-num_val_samples:]\n",
    "    for fname in val_files:\n",
    "        shutil.move(train_dir / category / fname,\n",
    "                    val_dir / category / fname)\n",
    "\n",
    "train_ds = keras.utils.text_dataset_from_directory(\n",
    "    \"aclImdb/train\", batch_size=batch_size\n",
    ")\n",
    "val_ds = keras.utils.text_dataset_from_directory(\n",
    "    \"aclImdb/val\", batch_size=batch_size\n",
    ")\n",
    "test_ds = keras.utils.text_dataset_from_directory(\n",
    "    \"aclImdb/test\", batch_size=batch_size\n",
    ")\n",
    "text_only_train_ds = train_ds.map(lambda x, y: x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "**Preparing integer sequence datasets**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "from tensorflow.keras import layers\n",
    "\n",
    "max_length = 600\n",
    "max_tokens = 20000\n",
    "text_vectorization = layers.TextVectorization(\n",
    "    max_tokens=max_tokens,\n",
    "    output_mode=\"int\",\n",
    "    output_sequence_length=max_length,\n",
    ")\n",
    "text_vectorization.adapt(text_only_train_ds)\n",
    "\n",
    "int_train_ds = train_ds.map(\n",
    "    lambda x, y: (text_vectorization(x), y),\n",
    "    num_parallel_calls=4)\n",
    "int_val_ds = val_ds.map(\n",
    "    lambda x, y: (text_vectorization(x), y),\n",
    "    num_parallel_calls=4)\n",
    "int_test_ds = test_ds.map(\n",
    "    lambda x, y: (text_vectorization(x), y),\n",
    "    num_parallel_calls=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "**A sequence model built on one-hot encoded vector sequences**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_3\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_4 (InputLayer)        [(None, None)]            0         \n",
      "                                                                 \n",
      " tf.one_hot_3 (TFOpLambda)   (None, None, 20000)       0         \n",
      "                                                                 \n",
      " bidirectional_3 (Bidirectio  (None, 64)               5128448   \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " dropout_3 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " dense_3 (Dense)             (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 5,128,513\n",
      "Trainable params: 5,128,513\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "inputs = keras.Input(shape=(None,), dtype=\"int64\")\n",
    "embedded = tf.one_hot(inputs, depth=max_tokens)\n",
    "x = layers.Bidirectional(layers.LSTM(32))(embedded)\n",
    "x = layers.Dropout(0.5)(x)\n",
    "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "model = keras.Model(inputs, outputs)\n",
    "model.compile(optimizer=\"rmsprop\",\n",
    "              loss=\"binary_crossentropy\",\n",
    "              metrics=[\"accuracy\"])\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "**Training a first basic sequence model**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/10\n",
      "1642/1642 [==============================] - 431s 261ms/step - loss: -57.1268 - accuracy: 0.0256 - val_loss: 57.4571 - val_accuracy: 0.4964\n",
      "Epoch 2/10\n",
      "1642/1642 [==============================] - 429s 262ms/step - loss: -154.7036 - accuracy: 0.0256 - val_loss: 110.1399 - val_accuracy: 0.4964\n",
      "Epoch 3/10\n",
      "1642/1642 [==============================] - 424s 258ms/step - loss: -251.7404 - accuracy: 0.0256 - val_loss: 162.8429 - val_accuracy: 0.4964\n",
      "Epoch 4/10\n",
      "1642/1642 [==============================] - 425s 259ms/step - loss: -348.6784 - accuracy: 0.0256 - val_loss: 215.5339 - val_accuracy: 0.4964\n",
      "Epoch 5/10\n",
      "1642/1642 [==============================] - 407s 248ms/step - loss: -446.5748 - accuracy: 0.0256 - val_loss: 268.2218 - val_accuracy: 0.4964\n",
      "Epoch 6/10\n",
      "1642/1642 [==============================] - 388s 236ms/step - loss: -542.8662 - accuracy: 0.0256 - val_loss: 320.8968 - val_accuracy: 0.4964\n",
      "Epoch 7/10\n",
      "1642/1642 [==============================] - 388s 236ms/step - loss: -640.8171 - accuracy: 0.0256 - val_loss: 373.5883 - val_accuracy: 0.4964\n",
      "Epoch 8/10\n",
      "1642/1642 [==============================] - 388s 236ms/step - loss: -738.3475 - accuracy: 0.0256 - val_loss: 426.2721 - val_accuracy: 0.4964\n",
      "Epoch 9/10\n",
      "1642/1642 [==============================] - 387s 236ms/step - loss: -835.7131 - accuracy: 0.0256 - val_loss: 478.9698 - val_accuracy: 0.4964\n",
      "Epoch 10/10\n",
      "1642/1642 [==============================] - 388s 236ms/step - loss: -932.0004 - accuracy: 0.0256 - val_loss: 531.6586 - val_accuracy: 0.4964\n",
      "782/782 [==============================] - 83s 106ms/step - loss: 57.0509 - accuracy: 0.5000\n",
      "Test acc: 0.500\n"
     ]
    }
   ],
   "source": [
    "callbacks = [\n",
    "    keras.callbacks.ModelCheckpoint(\"one_hot_bidir_lstm.keras\",\n",
    "                                    save_best_only=True)\n",
    "]\n",
    "model.fit(int_train_ds, validation_data=int_val_ds, epochs=10, callbacks=callbacks)\n",
    "model = keras.models.load_model(\"one_hot_bidir_lstm.keras\")\n",
    "print(f\"Test acc: {model.evaluate(int_test_ds)[1]:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "#### Understanding word embeddings"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "#### Learning word embeddings with the Embedding layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "**Instantiating an `Embedding` layer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "embedding_layer = layers.Embedding(input_dim=max_tokens, output_dim=256)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "**Model that uses an `Embedding` layer trained from scratch**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_4\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_5 (InputLayer)        [(None, None)]            0         \n",
      "                                                                 \n",
      " embedding_1 (Embedding)     (None, None, 256)         5120000   \n",
      "                                                                 \n",
      " bidirectional_4 (Bidirectio  (None, 64)               73984     \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " dropout_4 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " dense_4 (Dense)             (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 5,194,049\n",
      "Trainable params: 5,194,049\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "1642/1642 [==============================] - 118s 71ms/step - loss: -58.4677 - accuracy: 0.0256 - val_loss: 58.1442 - val_accuracy: 0.4964\n",
      "Epoch 2/10\n",
      "1642/1642 [==============================] - 116s 71ms/step - loss: -155.7627 - accuracy: 0.0256 - val_loss: 110.8350 - val_accuracy: 0.4964\n",
      "Epoch 3/10\n",
      "1642/1642 [==============================] - 116s 71ms/step - loss: -253.0585 - accuracy: 0.0256 - val_loss: 163.5160 - val_accuracy: 0.4964\n",
      "Epoch 4/10\n",
      "1642/1642 [==============================] - 117s 71ms/step - loss: -350.5391 - accuracy: 0.0256 - val_loss: 216.1997 - val_accuracy: 0.4964\n",
      "Epoch 5/10\n",
      "1642/1642 [==============================] - 115s 70ms/step - loss: -447.9632 - accuracy: 0.0256 - val_loss: 268.8857 - val_accuracy: 0.4964\n",
      "Epoch 6/10\n",
      "1642/1642 [==============================] - 118s 72ms/step - loss: -544.3305 - accuracy: 0.0256 - val_loss: 321.5734 - val_accuracy: 0.4964\n",
      "Epoch 7/10\n",
      "1642/1642 [==============================] - 119s 72ms/step - loss: -641.7681 - accuracy: 0.0256 - val_loss: 374.2687 - val_accuracy: 0.4964\n",
      "Epoch 8/10\n",
      "1642/1642 [==============================] - 117s 71ms/step - loss: -740.8792 - accuracy: 0.0256 - val_loss: 426.9545 - val_accuracy: 0.4964\n",
      "Epoch 9/10\n",
      "1642/1642 [==============================] - 119s 73ms/step - loss: -837.1086 - accuracy: 0.0256 - val_loss: 479.6389 - val_accuracy: 0.4964\n",
      "Epoch 10/10\n",
      "1642/1642 [==============================] - 119s 73ms/step - loss: -933.1600 - accuracy: 0.0256 - val_loss: 532.3347 - val_accuracy: 0.4964\n",
      "782/782 [==============================] - 23s 29ms/step - loss: 57.7332 - accuracy: 0.5000\n",
      "Test acc: 0.500\n"
     ]
    }
   ],
   "source": [
    "inputs = keras.Input(shape=(None,), dtype=\"int64\")\n",
    "embedded = layers.Embedding(input_dim=max_tokens, output_dim=256)(inputs)\n",
    "x = layers.Bidirectional(layers.LSTM(32))(embedded)\n",
    "x = layers.Dropout(0.5)(x)\n",
    "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "model = keras.Model(inputs, outputs)\n",
    "model.compile(optimizer=\"rmsprop\",\n",
    "              loss=\"binary_crossentropy\",\n",
    "              metrics=[\"accuracy\"])\n",
    "model.summary()\n",
    "\n",
    "callbacks = [\n",
    "    keras.callbacks.ModelCheckpoint(\"embeddings_bidir_gru.keras\",\n",
    "                                    save_best_only=True)\n",
    "]\n",
    "model.fit(int_train_ds, validation_data=int_val_ds, epochs=10, callbacks=callbacks)\n",
    "model = keras.models.load_model(\"embeddings_bidir_gru.keras\")\n",
    "print(f\"Test acc: {model.evaluate(int_test_ds)[1]:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "#### Understanding padding and masking"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "**Using an `Embedding` layer with masking enabled**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_5\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_6 (InputLayer)        [(None, None)]            0         \n",
      "                                                                 \n",
      " embedding_2 (Embedding)     (None, None, 256)         5120000   \n",
      "                                                                 \n",
      " bidirectional_5 (Bidirectio  (None, 64)               73984     \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " dropout_5 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " dense_5 (Dense)             (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 5,194,049\n",
      "Trainable params: 5,194,049\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "1642/1642 [==============================] - 138s 81ms/step - loss: -57.5860 - accuracy: 0.0256 - val_loss: 57.6912 - val_accuracy: 0.4964\n",
      "Epoch 2/10\n",
      "1642/1642 [==============================] - 130s 79ms/step - loss: -155.0746 - accuracy: 0.0256 - val_loss: 110.3775 - val_accuracy: 0.4964\n",
      "Epoch 3/10\n",
      "1642/1642 [==============================] - 129s 78ms/step - loss: -252.3633 - accuracy: 0.0256 - val_loss: 163.0663 - val_accuracy: 0.4964\n",
      "Epoch 4/10\n",
      "1642/1642 [==============================] - 129s 79ms/step - loss: -349.6462 - accuracy: 0.0256 - val_loss: 215.7546 - val_accuracy: 0.4964\n",
      "Epoch 5/10\n",
      "1642/1642 [==============================] - 130s 79ms/step - loss: -447.0052 - accuracy: 0.0256 - val_loss: 268.4512 - val_accuracy: 0.4964\n",
      "Epoch 6/10\n",
      "1642/1642 [==============================] - 131s 80ms/step - loss: -543.9589 - accuracy: 0.0256 - val_loss: 321.1420 - val_accuracy: 0.4964\n",
      "Epoch 7/10\n",
      "1642/1642 [==============================] - 129s 78ms/step - loss: -642.0096 - accuracy: 0.0256 - val_loss: 373.8434 - val_accuracy: 0.4964\n",
      "Epoch 8/10\n",
      "1642/1642 [==============================] - 130s 79ms/step - loss: -738.0376 - accuracy: 0.0256 - val_loss: 426.5436 - val_accuracy: 0.4964\n",
      "Epoch 9/10\n",
      "1642/1642 [==============================] - 127s 77ms/step - loss: -835.7902 - accuracy: 0.0256 - val_loss: 479.2403 - val_accuracy: 0.4964\n",
      "Epoch 10/10\n",
      "1642/1642 [==============================] - 129s 79ms/step - loss: -932.6627 - accuracy: 0.0256 - val_loss: 531.9188 - val_accuracy: 0.4964\n",
      "782/782 [==============================] - 27s 31ms/step - loss: 57.2832 - accuracy: 0.5000\n",
      "Test acc: 0.500\n"
     ]
    }
   ],
   "source": [
    "inputs = keras.Input(shape=(None,), dtype=\"int64\")\n",
    "embedded = layers.Embedding(\n",
    "    input_dim=max_tokens, output_dim=256, mask_zero=True)(inputs)\n",
    "x = layers.Bidirectional(layers.LSTM(32))(embedded)\n",
    "x = layers.Dropout(0.5)(x)\n",
    "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "model = keras.Model(inputs, outputs)\n",
    "model.compile(optimizer=\"rmsprop\",\n",
    "              loss=\"binary_crossentropy\",\n",
    "              metrics=[\"accuracy\"])\n",
    "model.summary()\n",
    "\n",
    "callbacks = [\n",
    "    keras.callbacks.ModelCheckpoint(\"embeddings_bidir_gru_with_masking.keras\",\n",
    "                                    save_best_only=True)\n",
    "]\n",
    "model.fit(int_train_ds, validation_data=int_val_ds, epochs=10, callbacks=callbacks)\n",
    "model = keras.models.load_model(\"embeddings_bidir_gru_with_masking.keras\")\n",
    "print(f\"Test acc: {model.evaluate(int_test_ds)[1]:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "#### Using pretrained word embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "# !wget http://nlp.stanford.edu/data/glove.6B.zip\n",
    "# !unzip -q glove.6B.zip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "**Parsing the GloVe word-embeddings file**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 400000 word vectors.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "path_to_glove_file = \"glove.6B.100d.txt\"\n",
    "\n",
    "embeddings_index = {}\n",
    "with open(path_to_glove_file,encoding=\"utf-8\") as f:\n",
    "    for line in f:\n",
    "        word, coefs = line.split(maxsplit=1)\n",
    "        coefs = np.fromstring(coefs, \"f\", sep=\" \")\n",
    "        embeddings_index[word] = coefs\n",
    "\n",
    "print(f\"Found {len(embeddings_index)} word vectors.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "**Preparing the GloVe word-embeddings matrix**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "embedding_dim = 100\n",
    "\n",
    "vocabulary = text_vectorization.get_vocabulary()\n",
    "word_index = dict(zip(vocabulary, range(len(vocabulary))))\n",
    "\n",
    "embedding_matrix = np.zeros((max_tokens, embedding_dim))\n",
    "for word, i in word_index.items():\n",
    "    if i < max_tokens:\n",
    "        embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        embedding_matrix[i] = embedding_vector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [],
   "source": [
    "embedding_layer = layers.Embedding(\n",
    "    max_tokens,\n",
    "    embedding_dim,\n",
    "    embeddings_initializer=keras.initializers.Constant(embedding_matrix),\n",
    "    trainable=False,\n",
    "    mask_zero=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text"
   },
   "source": [
    "**Model that uses a pretrained Embedding layer**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "colab_type": "code"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_6\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_7 (InputLayer)        [(None, None)]            0         \n",
      "                                                                 \n",
      " embedding_3 (Embedding)     (None, None, 100)         2000000   \n",
      "                                                                 \n",
      " bidirectional_6 (Bidirectio  (None, 64)               34048     \n",
      " nal)                                                            \n",
      "                                                                 \n",
      " dropout_6 (Dropout)         (None, 64)                0         \n",
      "                                                                 \n",
      " dense_6 (Dense)             (None, 1)                 65        \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 2,034,113\n",
      "Trainable params: 34,113\n",
      "Non-trainable params: 2,000,000\n",
      "_________________________________________________________________\n",
      "Epoch 1/10\n",
      "1642/1642 [==============================] - 145s 85ms/step - loss: -58.3398 - accuracy: 0.0256 - val_loss: 58.0670 - val_accuracy: 0.4964\n",
      "Epoch 2/10\n",
      "1642/1642 [==============================] - 134s 81ms/step - loss: -155.5929 - accuracy: 0.0256 - val_loss: 110.7396 - val_accuracy: 0.4964\n",
      "Epoch 3/10\n",
      "1642/1642 [==============================] - 134s 81ms/step - loss: -253.1330 - accuracy: 0.0256 - val_loss: 163.4381 - val_accuracy: 0.4964\n",
      "Epoch 4/10\n",
      "1642/1642 [==============================] - 134s 81ms/step - loss: -350.1617 - accuracy: 0.0256 - val_loss: 216.1243 - val_accuracy: 0.4964\n",
      "Epoch 5/10\n",
      "1642/1642 [==============================] - 129s 78ms/step - loss: -447.8175 - accuracy: 0.0256 - val_loss: 268.8282 - val_accuracy: 0.4964\n",
      "Epoch 6/10\n",
      "1642/1642 [==============================] - 125s 76ms/step - loss: -544.8859 - accuracy: 0.0256 - val_loss: 321.5348 - val_accuracy: 0.4964\n",
      "Epoch 7/10\n",
      "1642/1642 [==============================] - 125s 76ms/step - loss: -642.1803 - accuracy: 0.0256 - val_loss: 374.2243 - val_accuracy: 0.4964\n",
      "Epoch 8/10\n",
      "1642/1642 [==============================] - 124s 76ms/step - loss: -739.9490 - accuracy: 0.0256 - val_loss: 426.9129 - val_accuracy: 0.4964\n",
      "Epoch 9/10\n",
      "1642/1642 [==============================] - 128s 78ms/step - loss: -836.0649 - accuracy: 0.0256 - val_loss: 479.6027 - val_accuracy: 0.4964\n",
      "Epoch 10/10\n",
      "1642/1642 [==============================] - 125s 76ms/step - loss: -933.7882 - accuracy: 0.0256 - val_loss: 532.2949 - val_accuracy: 0.4964\n",
      "782/782 [==============================] - 27s 32ms/step - loss: 57.6543 - accuracy: 0.5000\n",
      "Test acc: 0.500\n"
     ]
    }
   ],
   "source": [
    "inputs = keras.Input(shape=(None,), dtype=\"int64\")\n",
    "embedded = embedding_layer(inputs)\n",
    "x = layers.Bidirectional(layers.LSTM(32))(embedded)\n",
    "x = layers.Dropout(0.5)(x)\n",
    "outputs = layers.Dense(1, activation=\"sigmoid\")(x)\n",
    "model = keras.Model(inputs, outputs)\n",
    "model.compile(optimizer=\"rmsprop\",\n",
    "              loss=\"binary_crossentropy\",\n",
    "              metrics=[\"accuracy\"])\n",
    "model.summary()\n",
    "\n",
    "callbacks = [\n",
    "    keras.callbacks.ModelCheckpoint(\"glove_embeddings_sequence_model.keras\",\n",
    "                                    save_best_only=True)\n",
    "]\n",
    "model.fit(int_train_ds, validation_data=int_val_ds, epochs=10, callbacks=callbacks)\n",
    "model = keras.models.load_model(\"glove_embeddings_sequence_model.keras\")\n",
    "print(f\"Test acc: {model.evaluate(int_test_ds)[1]:.3f}\")"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [],
   "name": "chapter11_part02_sequence-models.i",
   "private_outputs": false,
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  },
  "vscode": {
   "interpreter": {
    "hash": "09e93fcdc68a1dca2cb4d9ea696aa4bec5f2f761ad83a8a7e06f4cf4922a393b"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
